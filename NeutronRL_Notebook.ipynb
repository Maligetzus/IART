{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neutron - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "* Filipe Barbosa - up201909573\n",
    "* Hrvoje Maligec - up201911221\n",
    "* Jos√© Pedro Baptista - up201705255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this project, we made a self-learning agent using reinforcement learning techniques that plays the Neutron board game. In order to implement said agent, we used Python, and OpenAi Gym, an open source library for developing and comparing reinforcement learning algorithms, as we were advised to.\n",
    "\n",
    "### About the game\n",
    "\n",
    "#### Name\n",
    "Neutron\n",
    "\n",
    "#### Basic knowledge\n",
    "This game can be played in a 5x5 or 7x7 board.\n",
    "There are two kinds of pieces, one for each player (e.g. blue and red), and the neutron.\n",
    "\n",
    "#### Objective\n",
    "The objective of this game is to bring the Neutron (N piece) to their home row/base (row where all the players pieces start) or to stalemate the opponent (preventing the opponent from playing their turn because of lack of options). \n",
    "\n",
    "#### Rules\n",
    "The game starts with all blue pieces on Blue's home row and all red pieces on Red's home row, and the neutron in the center;\n",
    "All pieces (including the neutron) move in a straight line horizontally, vertically, or diagonally, but they must move as far as they can go in the chosen direction. They can only move through or onto empty squares, and never past any piece;\n",
    "Play begins with one player moving a piece from the home row. Thereafter on each turn, a player moves first the neutron and then one of his/her pieces.\n",
    "\n",
    "#### Implementation Limitations\n",
    "We don't support other boards other than the 5x5 one, due to the nature of this game and the way we are handling the different states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Description of the problem\n",
    "\n",
    "### Game representation\n",
    "\n",
    "#### Game State / Game Board\n",
    "For state representation, we are using an integer matrix (array of arrays of integers), and each one of them can represent a Blue piece, Red piece, Neutron or an empty tile. \n",
    "\n",
    "#### Pieces\n",
    "Each piece (or lack of) is represented by an integer value.\n",
    "\n",
    "* Empty tile = 0\n",
    "* Blue Piece = 1\n",
    "* Red Piece = 2\n",
    "* Neutron = 3\n",
    "\n",
    "#### Moves\n",
    "There are 8 possible move directions: Right, Left, Up, Down, Right-Up, Right-Down, Left-Up, Left-Down. In order to represent a move, we also need to specify the piece to apply the move to. So, our moves have the following representation:\n",
    "(piece_to_move_x, piece_to_move_y, direction)\n",
    "\n",
    "\n",
    "### What we needed to do\n",
    "In order to make the self-learning agent with Open Ai gym, we decided to make our own, specialised environment. For that, we had to (between other things):\n",
    "\n",
    "1. Define the observation and action spaces;\n",
    "2. Allow different render modes (besides pygame), since it would slow the episodes down, and that isn't convenient while the agent is learning;\n",
    "3. Incorporate CPU plays in the environment;\n",
    "4. Implement a step function that returns the new state, the reward, and whether the game has ended or not, given a move.\n",
    "After the environment was ready, we only had to implement the reinforcement learning algorithms to make the agent learn how to play neutron, given what the environment returned, after each action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Approach\n",
    "\n",
    "To implement the OpenAI Gym environment of the Neutron game, we used an [implementation we did for a previous projects](NeutronGame), that uses the Minimax algorithm for the CPU opponents. For the training, we use some of those bots as opponents for our learning bot.\n",
    "\n",
    "Since the Neutron game board has a big number of possible states (***GET NUMBER OF POSSIBLE STATES***), it was important that we only represented the valid ones, so as not to overbear the memory with unnecessary state representations that would never be reached. For that effect, we decided to start with an empty Q-table (for every algorithm) and only had a line for a state when that state was reached for the first time. Since the Q-table always starts with null values for each action-state pair, we could just not include those cases in it. As soon as a state was reached, it is then added to the Q-table.\n",
    "\n",
    "We decided to implement the **Q-learning algorithm** and the **State-action-reward-state-action (SARSA) algorithm** to use in our reinforcement learning bot.\n",
    "\n",
    "For both algorithms ***TALK ABOUT THE EPSILON DECAY***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Q-learning\n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. SARSA\n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimental Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Q-learning\n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Score over time: -0.066\nTime: 2.8877322673797607 seconds\n"
    }
   ],
   "source": [
    "import time\n",
    "from NeutronRL.q_learning import QLearning\n",
    "from NeutronRL.env_algorithm import EpsilonDecay\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "qlearning = QLearning(max_episodes=1000, max_steps=2000, epsilon_decay=EpsilonDecay.Exponential, env='Neutron-5x5-White-Random-v0', log=False)\n",
    "score, reward = qlearning.train()\n",
    "\n",
    "print(f\"Time: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Score over time: -0.006\nTime: 144.3859941959381 seconds\n"
    }
   ],
   "source": [
    "import time\n",
    "from NeutronRL.q_learning import QLearning\n",
    "from NeutronRL.env_algorithm import EpsilonDecay\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "qlearning = QLearning(max_episodes=1000, max_steps=2000, epsilon_decay=EpsilonDecay.Linear, env='Neutron-5x5-White-Random-v0', log=False)\n",
    "score, reward = qlearning.train()\n",
    "\n",
    "print(f\"Time: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Score over time: -0.604\nTime: 318.19483399391174 seconds\n"
    }
   ],
   "source": [
    "import time\n",
    "from NeutronRL.q_learning import QLearning\n",
    "from NeutronRL.env_algorithm import EpsilonDecay\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "qlearning = QLearning(max_episodes=1000, max_steps=2000, epsilon_decay=EpsilonDecay.Exponential, env='Neutron-5x5-White-Easy-v0', log=False)\n",
    "score, reward = qlearning.train()\n",
    "\n",
    "print(f\"Time: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Score over time: -0.008\nTime: 294.999986410141 seconds\n"
    }
   ],
   "source": [
    "import time\n",
    "from NeutronRL.q_learning import QLearning\n",
    "from NeutronRL.env_algorithm import EpsilonDecay\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "qlearning = QLearning(max_episodes=1000, max_steps=2000, epsilon_decay=EpsilonDecay.Linear, env='Neutron-5x5-White-Easy-v0', log=False)\n",
    "score, reward = qlearning.train()\n",
    "\n",
    "print(f\"Time: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. SARSA\n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions\n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "***TODO***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}