{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neutron - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "* Filipe Barbosa - up201909573\n",
    "* Hrvoje Maligec - up201911221\n",
    "* Jos√© Pedro Baptista - up201705255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this project, we made a self-learning agent using reinforcement learning techniques that plays the Neutron board game. In order to implement said agent, we used Python, and OpenAi Gym, an open source library for developing and comparing reinforcement learning algorithms, as we were advised to.\n",
    "\n",
    "### About the game\n",
    "\n",
    "#### Name\n",
    "Neutron\n",
    "\n",
    "#### Basic knowledge\n",
    "This game can be played in a 5x5 or 7x7 board.\n",
    "There are two kinds of pieces, one for each player (e.g. blue and red), and the neutron.\n",
    "\n",
    "#### Objective\n",
    "The objective of this game is to bring the Neutron (N piece) to their home row/base (row where all the players pieces start) or to stalemate the opponent (preventing the opponent from playing their turn because of lack of options). \n",
    "\n",
    "#### Rules\n",
    "The game starts with all blue pieces on Blue's home row and all red pieces on Red's home row, and the neutron in the center;\n",
    "All pieces (including the neutron) move in a straight line horizontally, vertically, or diagonally, but they must move as far as they can go in the chosen direction. They can only move through or onto empty squares, and never past any piece;\n",
    "Play begins with one player moving a piece from the home row. Thereafter on each turn, a player moves first the neutron and then one of his/her pieces.\n",
    "\n",
    "#### Implementation Limitations\n",
    "We don't support other boards other than the 5x5 one, due to the nature of this game and the way we are handling the different states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Description of the problem\n",
    "\n",
    "### Game representation\n",
    "\n",
    "#### Game State / Game Board\n",
    "For state representation, we are using an integer matrix (array of arrays of integers), and each one of them can represent a Blue piece, Red piece, Neutron or an empty tile. \n",
    "\n",
    "#### Pieces\n",
    "Each piece (or lack of) is represented by an integer value.\n",
    "Empty tile = 0\n",
    "Blue Piece = 1\n",
    "Red Piece = 2\n",
    "Neutron = 3\n",
    "\n",
    "#### Moves\n",
    "There are 8 possible move directions: Right, Left, Up, Down, Right-Up, Right-Down, Left-Up, Left-Down. In order to represent a move, we also need to specify the piece to apply the move to. So, our moves have the following representation:\n",
    "(piece_to_move_x, piece_to_move_y, direction)\n",
    "\n",
    "\n",
    "### What we needed to do\n",
    "In order to make the self-learning agent with Open Ai gym, we decided to make our own, specialised environment. For that, we had to (between other things):\n",
    "1. Define the observation and action spaces;\n",
    "2. Allow different render modes (besides pygame), since it would slow the episodes down, and that isn't convenient while the agent is learning;\n",
    "3. Incorporate CPU plays in the environment;\n",
    "4. Implement a step function that returns the new state, the reward, and whether the game has ended or not, given a move.\n",
    "After the environment was ready, we only had to implement the reinforcement learning algorithms to make the agent learn how to play neutron, given what the environment returned, after each action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Approach\n",
    "\n",
    "***TODO***\n",
    "\n",
    "\n",
    "### 3.1. Qlearning\n",
    "\n",
    "***TODO***\n",
    "\n",
    "\n",
    "### 3.2. SARSA\n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimental Evaluation\n",
    "\n",
    "\n",
    "### 4.1. Qlearning\n",
    "\n",
    "***TODO***\n",
    "\n",
    "### 4.2. SARSA\n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions\n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "***TODO***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
