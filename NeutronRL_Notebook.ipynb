{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neutron - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "* Filipe Barbosa - up201909573\n",
    "* Hrvoje Maligec - up201911221\n",
    "* Jos√© Pedro Baptista - up201705255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this project, we made a self-learning agent using reinforcement learning techniques that plays the Neutron board game. In order to implement said agent, we used Python, and OpenAi Gym, an open source library for developing and comparing reinforcement learning algorithms, as we were advised to.\n",
    "\n",
    "### About the game\n",
    "\n",
    "#### Name\n",
    "Neutron\n",
    "\n",
    "#### Basic knowledge\n",
    "This game can be played in a 5x5 or 7x7 board.\n",
    "There are two kinds of pieces, one for each player (e.g. blue and red), and the neutron.\n",
    "\n",
    "#### Objective\n",
    "The objective of this game is to bring the Neutron (N piece) to their home row/base (row where all the players pieces start) or to stalemate the opponent (preventing the opponent from playing their turn because of lack of options). \n",
    "\n",
    "#### Rules\n",
    "The game starts with all blue pieces on Blue's home row and all red pieces on Red's home row, and the neutron in the center;\n",
    "All pieces (including the neutron) move in a straight line horizontally, vertically, or diagonally, but they must move as far as they can go in the chosen direction. They can only move through or onto empty squares, and never past any piece;\n",
    "Play begins with one player moving a piece from the home row. Thereafter on each turn, a player moves first the neutron and then one of his/her pieces.\n",
    "\n",
    "#### Implementation Limitations\n",
    "We don't support other boards other than the 5x5 one, due to the nature of this game and the way we are handling the different states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Description of the problem\n",
    "\n",
    "For this project, we used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Approach\n",
    "\n",
    "To implement the OpenAI Gym environment of the Neutron game, we used an [implementation we did for a previous projects](NeutronGame), that uses the Minimax algorithm for the CPU opponents. For the training, we use some of those bots as opponents for our learning bot.\n",
    "\n",
    "Since the Neutron game board has a big number of possible states (***GET NUMBER OF POSSIBLE STATES***), it was important that we only represented the valid ones, so as not to overbear the memory with unnecessary state representations that would never be reached. For that effect, we decided to start with an empty Q-table (for every algorithm) and only had a line for a state when that state was reached for the first time. Since the Q-table always starts with null values for each action-state pair, we could just not include those cases in it. As soon as a state was reached, it is then added to the Q-table.\n",
    "\n",
    "We decided to implement the **Q-learning algorithm** and the **State-action-reward-state-action (SARSA) algorithm** to use in our reinforcement learning bot.\n",
    "\n",
    "For both algorithms ***TALK ABOUT THE EPSILON DECAY***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Q-learning\n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. SARSA\n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimental Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Q-learning\n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Score over time: -0.009\n"
    }
   ],
   "source": [
    "from NeutronRL.q_learning import QLearning\n",
    "from NeutronRL.env_algorithm import EpsilonDecay\n",
    "\n",
    "qlearning = QLearning(max_episodes=1000, max_steps=2000, epsilon_decay=EpsilonDecay.Linear, env='Neutron-5x5-White-Easy-v0', log=False)\n",
    "score, reward = qlearning.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. SARSA\n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions\n",
    "\n",
    "***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "***TODO***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}